# Process Book: Marine Diversity Dive

## Overview and Motivation
The ocean covers 70% of our planet, yet the deep sea remains largely unexplored. This project, "Marine Diversity Dive," aims to visualize the distribution of marine biodiversity across different depth zonesâ€”from the sunlit Epipelagic zone to the dark Abyssopelagic zone. By leveraging data from the Ocean Biodiversity Information System (OBIS), we seek to reveal patterns in species composition and ecological communities that are often hidden beneath the surface. The motivation is to make complex biodiversity data accessible and engaging through an interactive "dive" experience, allowing users to see not just where marine life is, but how it changes as they descend.

## Related Work
*   **OBIS (Ocean Biodiversity Information System)**: The primary source of our data and an inspiration for global-scale marine mapping.
*   **"The Deep Sea" by Neal.fun**: Inspired the vertical scrolling interaction to represent depth.
*   
## Questions
1.  **How does species composition change with depth?** We wanted to see if the dominant species in the surface layer are completely different from those in the abyss.
2.  **Can we identify distinct ecological communities?** Rather than just mapping individual species, can we group them into "communities" based on co-occurrence?
3.  **How are these communities distributed globally?** Are deep-sea communities more uniform across the globe compared to surface communities?

*Evolution*: Initially, we asked simple questions about abundance (e.g., "Where are the most fish?"). However, we realized abundance is heavily biased by sampling effort. We shifted our questions to focus on *composition* and *community structure* using clustering algorithms, which provides a more robust view of biodiversity patterns.

## Data

### Source
Data is sourced from the **OBIS (Ocean Biodiversity Information System)** dataset. We accessed the data directly from their S3 bucket using **DuckDB** and R (`data/pull_data.r`). This allowed us to query massive parquet files efficiently without downloading terabytes of data.

### Processing
Data processing steps are handled in `data/model_biodiversity.r`:

1.  **Filtering**: We filtered for records from 2010 onwards, specifically Kingdoms *Animalia* and *Plantae*. We removed records with missing depth or coordinates.
2.  **Binning**: Data was aggregated into spatial bins (0.5-degree grid) and four depth layers:
    *   Epipelagic Zone (0-200m)
    *   Mesopelagic Zone (200-1000m)
    *   Bathypelagic Zone (1000-4000m)
    *   Abyssopelagic Zone (4000m+)
3.  **Clustering**: We used **K-Means clustering** (k=12 per zone) on Hellinger-transformed abundance matrices to identify distinct ecological communities.
4.  **Interpolation**: To create a continuous map from sparse sampling points, we used **K-Nearest Neighbors (KNN)**. We converted lat/lon to 3D Cartesian coordinates (X, Y, Z) to perform spherical interpolation, ensuring accuracy near the poles.
5.  **Bathymetry Masking**: We used NOAA bathymetry data to mask out areas where the seafloor is shallower than the depth zone being modeled (e.g., preventing deep-sea predictions on continental shelves).

### Dataset Structure
Below are samples of the processed datasets generated by our pipeline.

#### 1. `predicted_communities.csv`
This file drives the global map visualization. It contains the predicted ecological community for every grid point in the ocean.

| lat_bin | lon_bin | depth_layer | predicted_community_id |
|:---|:---|:---|:---|
| 42.5 | -68.5 | Epipelagic Zone | 3 |
| 42.5 | -68.0 | Epipelagic Zone | 3 |
| -10.0 | 145.5 | Bathypelagic Zone | 15 |

*   **lat_bin / lon_bin**: The center coordinates of the 0.5-degree spatial grid cell.
*   **depth_layer**: The vertical zone (Epipelagic, Mesopelagic, etc.).
*   **predicted_community_id**: An integer ID representing the specific ecological cluster assigned to this location.

#### 2. `community_composition.csv`
This file provides the biological context for each community ID, listing the top species found within it.

| community_id | species | avg_abundance |
|:---|:---|:---|
| 3 | *Calanus finmarchicus* | 450.2 |
| 3 | *Meganyctiphanes norvegica* | 120.5 |
| 15 | *Cyclothone microdon* | 12.1 |

*   **community_id**: Matches the ID in the grid file.
*   **species**: The scientific name of the dominant species.
*   **avg_abundance**: The average number of individuals of this species found in samples assigned to this community.

#### 3. `raw_species_data.csv`
The aggregated raw data from OBIS before clustering.

| kingdom | genus | species | lat_bin | lon_bin | depth_layer | total_abundance |
|:---|:---|:---|:---|:---|:---|:---|
| Animalia | Thunnus | *Thunnus thynnus* | 35.5 | -75.0 | Epipelagic | 50 |

*   **total_abundance**: Sum of individual counts for that species in that bin/layer.

## Exploratory Data Analysis
We used `ggplot2` in R to generate static heatmaps of species abundance and preliminary cluster maps.

*   **Insight 1**: Raw abundance maps were extremely patchy, reflecting shipping lanes and research expeditions rather than true biological distribution. This informed the decision to use **KNN interpolation** to smooth the visual output.
*   **Insight 2**: Surface species (like tuna) dominated the dataset. If we analyzed all depths together, deep-sea signals were lost. This led to the design decision to **model each depth layer independently**, ensuring deep-sea communities were properly represented.
*   **Insight 3**: The bathymetry is crucial. Early visualizations showed "deep sea" communities on land. We added a bathymetry filter to strictly mask invalid depths.

## Design Evolution
### TODO

### Initial Sketches
*(Include photos of sketches here)*

### Prototypes
*(Include screenshots of early prototypes)*

## Implementation
*Describe the intent and functionality of the interactive visualizations you implemented. Provide clear and well-referenced images showing the key design and interaction elements.*

### Architecture
The project uses a static site structure with:
- **HTML/CSS/JS**: For the frontend interface (`index.html`, `main.js`, `style.css`).
- **D3.js (v7)**: For rendering the interactive map and handling data binding.
- **R**: For heavy data lifting, clustering, and generating the `predicted_communities.csv` and `community_composition.csv` files used by the frontend.

### Key Features
- **Global Map**: A D3-based map that visualizes the predicted communities. Users can click to select a specific coordinate.
- **Scrollytelling Dive**: As the user scrolls, an `IntersectionObserver` triggers transitions between depth zones:
  1. Epipelagic Zone (0-200m)
  2. Mesopelagic Zone (200-1000m)
  3. Bathypelagic Zone (1000-4000m)
  4. Abyssopelagic Zone (4000m+)
- **Sticky Visualization**: The visualization panel (`#vis-sticky`) remains fixed while text content scrolls over it, maintaining context.

## Evaluation
*   **Findings**: The visualization clearly shows that biodiversity "hotspots" shift as you go deeper. Surface hotspots don't always align with deep-sea hotspots.
*   **Performance**: The KNN model provides a good approximation, but the transition between zones can be abrupt.
*   **Future Improvements**:
    *   TODO

## References
- D3.js
- DuckDB
- R packages: `dplyr`, `tidyr`, `class`, `ggplot2`, `sf`, `marmap`
